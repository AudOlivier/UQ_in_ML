{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import keras.backend as K\n",
    "from keras import initializers\n",
    "from keras.engine import InputSpec\n",
    "from keras.layers import Dense, Lambda, Wrapper\n",
    "\n",
    "\n",
    "class ConcreteDropout(Wrapper):\n",
    "    \"\"\"This wrapper allows to learn the dropout probability for any given input Dense layer.\n",
    "    ```python\n",
    "        # as the first layer in a model\n",
    "        model = Sequential()\n",
    "        model.add(ConcreteDropout(Dense(8), input_shape=(16)))\n",
    "        # now model.output_shape == (None, 8)\n",
    "        # subsequent layers: no need for input_shape\n",
    "        model.add(ConcreteDropout(Dense(32)))\n",
    "        # now model.output_shape == (None, 32)\n",
    "    ```\n",
    "    `ConcreteDropout` can be used with arbitrary layers which have 2D\n",
    "    kernels, not just `Dense`. However, Conv2D layers require different\n",
    "    weighing of the regulariser (use SpatialConcreteDropout instead).\n",
    "    # Arguments\n",
    "        layer: a layer instance.\n",
    "        weight_regularizer:\n",
    "            A positive number which satisfies\n",
    "                $weight_regularizer = l**2 / (\\tau * N)$\n",
    "            with prior lengthscale l, model precision $\\tau$ (inverse observation noise),\n",
    "            and N the number of instances in the dataset.\n",
    "            Note that kernel_regularizer is not needed.\n",
    "        dropout_regularizer:\n",
    "            A positive number which satisfies\n",
    "                $dropout_regularizer = 2 / (\\tau * N)$\n",
    "            with model precision $\\tau$ (inverse observation noise) and N the number of\n",
    "            instances in the dataset.\n",
    "            Note the relation between dropout_regularizer and weight_regularizer:\n",
    "                $weight_regularizer / dropout_regularizer = l**2 / 2$\n",
    "            with prior lengthscale l. Note also that the factor of two should be\n",
    "            ignored for cross-entropy loss, and used only for the eculedian loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, weight_regularizer=1e-6, dropout_regularizer=1e-5,\n",
    "                 init_min=0.1, init_max=0.1, is_mc_dropout=True, **kwargs):\n",
    "        assert 'kernel_regularizer' not in kwargs\n",
    "        super(ConcreteDropout, self).__init__(layer, **kwargs)\n",
    "        self.weight_regularizer = weight_regularizer\n",
    "        self.dropout_regularizer = dropout_regularizer\n",
    "        self.is_mc_dropout = is_mc_dropout\n",
    "        self.supports_masking = True\n",
    "        self.p_logit = None\n",
    "        self.p = None\n",
    "        self.init_min = np.log(init_min) - np.log(1. - init_min)\n",
    "        self.init_max = np.log(init_max) - np.log(1. - init_max)\n",
    "\n",
    "    def build(self, input_shape=None):\n",
    "        self.input_spec = InputSpec(shape=input_shape)\n",
    "        if not self.layer.built:\n",
    "            self.layer.build(input_shape)\n",
    "            self.layer.built = True\n",
    "        super(ConcreteDropout, self).build()  # this is very weird.. we must call super before we add new losses\n",
    "\n",
    "        # initialise p\n",
    "        self.p_logit = self.layer.add_weight(name='p_logit',\n",
    "                                            shape=(1,),\n",
    "                                            initializer=initializers.RandomUniform(self.init_min, self.init_max),\n",
    "                                            trainable=True)\n",
    "        self.p = K.sigmoid(self.p_logit[0])\n",
    "\n",
    "        # initialise regulariser / prior KL term\n",
    "        assert len(input_shape) == 2, 'this wrapper only supports Dense layers'\n",
    "        input_dim = np.prod(input_shape[-1])  # we drop only last dim\n",
    "        weight = self.layer.kernel\n",
    "        kernel_regularizer = self.weight_regularizer * K.sum(K.square(weight)) / (1. - self.p)\n",
    "        dropout_regularizer = self.p * K.log(self.p)\n",
    "        dropout_regularizer += (1. - self.p) * K.log(1. - self.p)\n",
    "        dropout_regularizer *= self.dropout_regularizer * input_dim\n",
    "        regularizer = K.sum(kernel_regularizer + dropout_regularizer)\n",
    "        self.layer.add_loss(regularizer)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return self.layer.compute_output_shape(input_shape)\n",
    "\n",
    "    def concrete_dropout(self, x):\n",
    "        '''\n",
    "        Concrete dropout - used at training time (gradients can be propagated)\n",
    "        :param x: input\n",
    "        :return:  approx. dropped out input\n",
    "        '''\n",
    "        eps = K.cast_to_floatx(K.epsilon())\n",
    "        temp = 0.1\n",
    "\n",
    "        unif_noise = K.random_uniform(shape=K.shape(x))\n",
    "        drop_prob = (\n",
    "            K.log(self.p + eps)\n",
    "            - K.log(1. - self.p + eps)\n",
    "            + K.log(unif_noise + eps)\n",
    "            - K.log(1. - unif_noise + eps)\n",
    "        )\n",
    "        drop_prob = K.sigmoid(drop_prob / temp)\n",
    "        random_tensor = 1. - drop_prob\n",
    "\n",
    "        retain_prob = 1. - self.p\n",
    "        x *= random_tensor\n",
    "        x /= retain_prob\n",
    "        return x\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if self.is_mc_dropout:\n",
    "            return self.layer.call(self.concrete_dropout(inputs))\n",
    "        else:\n",
    "            def relaxed_dropped_inputs():\n",
    "                return self.layer.call(self.concrete_dropout(inputs))\n",
    "            return K.in_train_phase(relaxed_dropped_inputs,\n",
    "                                    self.layer.call(inputs),\n",
    "                                    training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functools import partial\n",
    "from UQ_in_ML.general_utils import *\n",
    "\n",
    "def set_problem(problem, n_data):\n",
    "    if problem == 'cubic':\n",
    "        var_n = 9.\n",
    "        f = partial(f_cubic, var_n=var_n)\n",
    "        xn = np.random.uniform(low=-4.5, high=4.5, size=(n_data,1))\n",
    "        bounds = (-6, 6)\n",
    "    elif problem == 'sin_cos':\n",
    "        var_n = 0.1**2\n",
    "        f = partial(f_sin_cos, var_n=var_n)\n",
    "        bounds = (0, 10)\n",
    "        xn = np.random.uniform(low=bounds[0], high=bounds[1], size=(n_data,1))\n",
    "    yn = f(xn, noisy=True)\n",
    "    x_plot = np.linspace(bounds[0], bounds[1], 100).reshape((-1,1))\n",
    "    fig, ax = plt.subplots(ncols=1, figsize=(6, 4))\n",
    "    y_plot = f(x_plot, noisy=False)\n",
    "    ax.plot(x_plot, y_plot, color='green', label='true function')\n",
    "    ax.plot(xn, yn, color='blue', marker='.', linestyle='none', label='data')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return f, var_n, bounds, xn, yn, x_plot, y_plot\n",
    "\n",
    "def plot_regressor(reg, x_plot, y_plot, xn, yn, title):\n",
    "    fig, ax = plt.subplots(ncols=1, figsize=(6, 4))\n",
    "    ax.plot(x_plot, y_plot, color='green', label='true', alpha=0.5)\n",
    "    plot_UQ(reg, X=x_plot, ax=ax, plot_one_posterior=True)\n",
    "    ax.plot(xn, yn, linestyle='none',marker='.',label='data')\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "    \n",
    "# Choose the problem, and the network architecture\n",
    "problem = 'sin_cos' #'cubic' or 'sin_cos'\n",
    "n_data = 20\n",
    "f, var_n, bounds, xn, yn, x_plot, y_plot = set_problem(problem, n_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Lambda, merge\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "precision = 1 / var_n\n",
    "nb_features = 10\n",
    "batch_size = n_data\n",
    "\n",
    "def fit_model(l, nb_epoch, X, Y):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        K.clear_session()\n",
    "    N = X.shape[0]\n",
    "    wd = l**2. / N\n",
    "    dd = 2. / N\n",
    "    inp = Input(shape=(1,))\n",
    "    x = inp\n",
    "    x = ConcreteDropout(Dense(nb_features, activation='relu'), weight_regularizer=wd, dropout_regularizer=dd)(x)\n",
    "    x = ConcreteDropout(Dense(nb_features, activation='relu'), weight_regularizer=wd, dropout_regularizer=dd)(x)\n",
    "    x = ConcreteDropout(Dense(nb_features, activation='relu'), weight_regularizer=wd, dropout_regularizer=dd)(x)\n",
    "    mean = ConcreteDropout(Dense(1), weight_regularizer=wd, dropout_regularizer=dd)(x)\n",
    "    model = Model(inp, mean)\n",
    "    \n",
    "    def homoscedastic_loss(true, pred):\n",
    "        return K.sum(precision * (true - pred)**2., -1)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=homoscedastic_loss)\n",
    "    assert len(model.layers[1].trainable_weights) == 3  # kernel, bias, and dropout prob\n",
    "    assert len(model.losses) == 4  # a loss for each Concrete Dropout layer\n",
    "    hist = model.fit(X, Y, nb_epoch=nb_epoch, batch_size=batch_size, verbose=0)\n",
    "    loss = hist.history['loss'][-1]\n",
    "    return model, -0.5 * loss  # return ELBO up to const."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.798266410827637\n",
      "[0.09782438]\n",
      "[0.0983275]\n",
      "-11.224080085754395\n",
      "[0.09803983]\n",
      "[0.09413876]\n"
     ]
    }
   ],
   "source": [
    "l = 1e-4\n",
    "\n",
    "nb_epoch = 100\n",
    "model, ELBO = fit_model(l, nb_epoch, xn, yn)\n",
    "print(ELBO)\n",
    "print(K.eval(K.sigmoid(model.layers[1].get_weights()[2])))\n",
    "print(K.eval(K.sigmoid(model.layers[2].get_weights()[2])))\n",
    "\n",
    "nb_epoch = 500\n",
    "model, ELBO = fit_model(l, nb_epoch, xn, yn)\n",
    "print(ELBO)\n",
    "print(K.eval(K.sigmoid(model.layers[1].get_weights()[2])))\n",
    "print(K.eval(K.sigmoid(model.layers[2].get_weights()[2])))\n",
    "\n",
    "nb_epoch = 2000\n",
    "model, ELBO = fit_model(l, nb_epoch, xn, yn)\n",
    "print(ELBO)\n",
    "print(K.eval(K.sigmoid(model.layers[1].get_weights()[2])))\n",
    "print(K.eval(K.sigmoid(model.layers[2].get_weights()[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 1e-2\n",
    "\n",
    "nb_epoch = 100\n",
    "model, ELBO = fit_model(l, nb_epoch, xn, yn)\n",
    "\n",
    "print(K.eval(K.sigmoid(model.layers[1].get_weights()[2])))\n",
    "print(K.eval(K.sigmoid(model.layers[2].get_weights()[2])))\n",
    "\n",
    "nb_epoch = 20000\n",
    "model, ELBO = fit_model(l, nb_epoch, xn, yn)\n",
    "print(K.eval(K.sigmoid(model.layers[1].get_weights()[2])))\n",
    "print(K.eval(K.sigmoid(model.layers[2].get_weights()[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
